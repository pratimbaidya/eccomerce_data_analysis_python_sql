# Eccomerce Data Analysis
In this repository I solve 15 different queries on a ecommerce  dataset using SQL and Python

## About the Dataset
Link to the dataset on Kaggle - [LINK](https://www.kaggle.com/datasets/devarajv88/target-dataset?select=payments.csv)

### Description of the Data
Target is a globally recognized brand and a leading retailer in the United States, known for offering exceptional value, inspiration, innovation, and a unique shopping experience.

This dataset focuses on Target's operations in Brazil, covering 100,000 orders placed between 2016 and 2018. It includes detailed information on order status, pricing, payment and shipping performance, customer locations, product attributes, and customer reviews.

**Features**
The data is available in 7 csv files:

- customers.csv
- sellers.csv
- order_items.csv
- payments.csv
- orders.csv
- products.csv
- geolocation.csv

Structure for each of these given below

**Table: customers**
The customers.csv contain following features:

1. customer_id: ID of the consumer who made the purchase
2. customer_unique_id: Unique ID of the consumer
3. customer_zip_code_prefix: Zip Code of consumer’s location
4. customer_city: Name of the City from where order is made
5. customer_state: State Code from where order is made (Eg. são paulo - SP)

**Table: sellers**
The sellers.csv contains following features:

1. seller_id: Unique ID of the seller registered
2. seller_zip_code_prefix: Zip Code of the seller’s location
3. seller_city: Name of the City of the seller
4. seller_state: State Code (Eg. são paulo - SP)

**Table: order_items**
The order_items.csv contain following features:

1. order_id: A Unique ID of order made by the consumers
2. order_item_id: A Unique ID given to each item ordered in the order
3. product_id: A Unique ID given to each product available on the site
4. seller_id: Unique ID of the seller registered in Target
5. shipping_limit_date: The date before which the ordered product must be shipped
6. price: Actual price of the products ordered
7. freight_value: Price rate at which a product is delivered from one point to another

**Table: payments**
The payments.csv contain following features:

1. order_id: A Unique ID of order made by the consumers
2. payment_sequential: Sequences of the payments made in case of EMI
3. payment_type: Mode of payment used (Eg. Credit Card)
4. payment_installments: Number of installments in case of EMI purchase
5. payment_value: Total amount paid for the purchase order

**Table: orders**
The orders.csv contain following features:

1. order_id: A Unique ID of order made by the consumers
2. customer_id: ID of the consumer who made the purchase
3. order_status: Status of the order made i.e. delivered, shipped, etc.
4. order_purchase_timestamp: Timestamp of the purchase
5. order_delivered_carrier_date: Delivery date at which carrier made the delivery
6. order_delivered_customer_date: Date at which customer got the product
7. order_estimated_delivery_date: Estimated delivery date of the products

**Table: products**
The products.csv contain following features:

1. product_id: A Unique identifier for the proposed project
2. product_category_name: Name of the product category
3. product_name_lenght: Length of the string which specifies the name given to the products ordered
4. product_description_length: Length of the description written for each product ordered on the site
5. product_photos_qty: Number of photos of each product ordered available on the shopping portal
6. product_weight_g: Weight of the products ordered in grams
7. product_length_cm: Length of the products ordered in centimeters
8. product_height_cm: Height of the products ordered in centimeters
9. product_width_cm: Width of the product ordered in centimeters

**Table: geolocation**
The geolocations.csv contain following features:

1. geolocation_zip_code_prefix: First 5 digits of Zip Code
2. geolocation_lat: Latitude
3. geolocation_lng: Longitude
4. geolocation_city: City
5. geolocation_state: State

## Queries that are answered in this project
**Basic Queries**
1. List all unique cities where customers are located.
2. Count the number of orders placed in 2017.
3. Find the total sales per category.
4. Calculate the percentage of orders that were paid in installments.
5. Count the number of customers from each state. 

**Intermediate Queries**
1. Calculate the number of orders per month in 2018.
2. Find the average number of products per order, grouped by customer city.
3. Calculate the percentage of total revenue contributed by each product category.
4. Identify the correlation between product price and the number of times a product has been purchased.
5. Calculate the total revenue generated by each seller, and rank them by revenue.

**Advanced Queries**
1. Calculate the moving average of order values for each customer over their order history.
2. Calculate the cumulative sales per month for each year.
3. Calculate the year-over-year growth rate of total sales.
4. Calculate the retention rate of customers, defined as the percentage of customers who make another purchase within 6 months of their first purchase.
5. Identify the top 3 customers who spent the most money in each year.

## Process
I ran the **csv_to_sql_dumping.ipynb** motebook to import the .csv files into a database in MySQL.
Then ran the queries in Python using MySQL in the **Analysis.ipynb notebook**. Used Pandas to do some basic manipulation on the results from the query if needed. Used Matplotlib and Seaborn to plot whereever needed.

## How to replicate

Make directory and use this structure

📁 Direcory

├── 📄 Analysis.ipynb

├── 📄 Quentions.txt

├── 📁 csv files

│   ├── 📄 customers.csv

│   ├── 📄 geolocation.csv

│   ├── 📄 order_items.csv

│   ├── 📄 orders.csv

│   ├── 📄 payments.csv

│   ├── 📄 products.csv

│   └── 📄 sellers.csv

└── 📄 csv_to_sql_dumping.ipynb

For both **csv_to_sql_dumping.ipynb** and **Analysis.ipynb notebook** change these MySQL settings according to your DataBase:
    - host = "localhost",
    - username = "root",
    - password = "your_password",
    - database = "ecommerce"

First run the **csv_to_sql_dumping.ipynb** notebook to import all the csv files into sql. It may take sometime. Then run the **Analysis.ipynb notebook** notebook to see all the results.
